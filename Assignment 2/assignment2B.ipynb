{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab Assignment 2 - Part B: k-Nearest Neighbor Classification\n",
        "Please refer to the `README.pdf` for full laboratory instructions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem Statement\n",
        "In this part, you will implement the k-Nearest Neighbor (k-NN) classifier and evaluate it on two datasets:\n",
        "- **Lenses Dataset**: A small dataset for contact lens prescription\n",
        "- **Credit Approval (CA) Dataset**: Credit card application data with binary labels (+/-)\n",
        "\n",
        "### Your Tasks\n",
        "1. **Preprocess the data**: Handle missing values and normalize features\n",
        "2. **Implement k-NN** with L2 distance\n",
        "3. **Evaluate** on both datasets for different values of k\n",
        "4. **Discuss** your results\n",
        "\n",
        "### Datasets\n",
        "The data files are located in the `credit 2017/` folder:\n",
        "- `lenses.training`, `lenses.testing`\n",
        "- `crx.data.training`, `crx.data.testing`\n",
        "- `crx.names` (describes the features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Library declarations\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lenses - Train: (18, 3), Test: (6, 3)\n"
          ]
        }
      ],
      "source": [
        "# Data paths\n",
        "DATA_PATH = \"credit 2017/\"\n",
        "\n",
        "# Load Lenses data\n",
        "def load_lenses_data():\n",
        "    \"\"\"Load the lenses dataset.\"\"\"\n",
        "    train_data = np.loadtxt(DATA_PATH + \"lenses.training\", delimiter=',')\n",
        "    test_data = np.loadtxt(DATA_PATH + \"lenses.testing\", delimiter=',')\n",
        "    \n",
        "    # First column is ID, last column is label\n",
        "    X_train = train_data[:, 1:-1]\n",
        "    y_train = train_data[:, -1]\n",
        "    X_test = test_data[:, 1:-1]\n",
        "    y_test = test_data[:, -1]\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "# Load Credit Approval data\n",
        "def load_credit_data():\n",
        "    \"\"\"\n",
        "    Load the Credit Approval dataset.\n",
        "    Note: This dataset contains missing values (?) and mixed types.\n",
        "    You will need to preprocess it.\n",
        "    \"\"\"\n",
        "    # TODO: Implement data loading\n",
        "    # The data is comma-separated\n",
        "    # Missing values are marked with '?'\n",
        "    # Last column is the label ('+' or '-')\n",
        "    train_file = DATA_PATH + \"crx.data.training\"\n",
        "    test_file = DATA_PATH + \"crx.data.testing\"\n",
        "\n",
        "    train_raw = np.genfromtxt(train_file, delimiter=',', dtype=str)\n",
        "    test_raw = np.genfromtxt(test_file, delimiter=',', dtype=str)\n",
        "\n",
        "    X_train = train_raw[:, :-1]\n",
        "    y_train = train_raw[:, -1]\n",
        "    X_test = test_raw[:, :-1]\n",
        "    y_test = test_raw[:, -1]\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "# Test loading lenses data\n",
        "X_train_lenses, y_train_lenses, X_test_lenses, y_test_lenses = load_lenses_data()\n",
        "print(f\"Lenses - Train: {X_train_lenses.shape}, Test: {X_test_lenses.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Data Preprocessing\n",
        "For the Credit Approval dataset, you need to:\n",
        "1. **Handle missing values** (marked with '?'):\n",
        "   - Categorical features: replace with mode/median\n",
        "   - Numerical features: replace with label-conditioned mean\n",
        "2. **Normalize features** using z-scaling:\n",
        "   $$z_i^{(m)} = \\frac{x_i^{(m)} - \\mu_i}{\\sigma_i}$$\n",
        "\n",
        "Document exactly how you handle each feature!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_credit_data(train_file, test_file):\n",
        "    \"\"\"\n",
        "    Preprocess the Credit Approval dataset.\n",
        "    \n",
        "    Steps:\n",
        "    1. Load the data\n",
        "    2. Handle missing values\n",
        "    3. Encode categorical variables\n",
        "    4. Normalize numerical features\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    X_train, y_train, X_test, y_test : numpy arrays\n",
        "    \"\"\"\n",
        "    # TODO: Implement preprocessing\n",
        "    # Hint: Read crx.names to understand the features\n",
        "    # Feature types (from crx.names):\n",
        "    # A1: categorical (b, a)\n",
        "    # A2: continuous\n",
        "    # A3: continuous\n",
        "    # A4: categorical (u, y, l, t)\n",
        "    # A5: categorical (g, p, gg)\n",
        "    # A6: categorical (c, d, cc, i, j, k, m, r, q, w, x, e, aa, ff)\n",
        "    # A7: categorical (v, h, bb, j, n, z, dd, ff, o)\n",
        "    # A8: continuous\n",
        "    # A9: categorical (t, f)\n",
        "    # A10: categorical (t, f)\n",
        "    # A11: continuous\n",
        "    # A12: categorical (t, f)\n",
        "    # A13: categorical (g, p, s)\n",
        "    # A14: continuous\n",
        "    # A15: continuous\n",
        "    # A16: label (+, -)\n",
        "    \n",
        "    # categorical features: mode\n",
        "    train_raw = np.genfromtxt(train_file, delimiter=',', dtype=str)\n",
        "    test_raw = np.genfromtxt(test_file, delimiter=',', dtype=str)\n",
        "\n",
        "    X_train_raw = train_raw[:, :-1]\n",
        "    y_train = train_raw[:, -1]\n",
        "    X_test_raw = test_raw[:, :-1]\n",
        "    y_test = test_raw[:, -1]\n",
        "\n",
        "    numerical_idx = [1, 2, 7, 10, 13, 14]\n",
        "    categorical_idx = [i for i in range(15) if i not in numerical_idx]\n",
        "\n",
        "    # Missing value imputation\n",
        "    # Categorical: replace '?' with mode (computed from training, per feature)\n",
        "    for j in categorical_idx:\n",
        "        col = X_train_raw[:, j]\n",
        "        non_missing = col[col != '?']\n",
        "        if len(non_missing) == 0:\n",
        "            mode_val = '0'\n",
        "        else:\n",
        "            mode_val = Counter(non_missing).most_common(1)[0][0]\n",
        "        X_train_raw[col == '?', j] = mode_val\n",
        "        X_test_raw[X_test_raw[:, j] == '?', j] = mode_val\n",
        "\n",
        "    # Numerical: replace '?' with label-conditioned mean from training\n",
        "    label_set = np.unique(y_train)\n",
        "    class_means = {lab: {} for lab in label_set}\n",
        "    for j in numerical_idx:\n",
        "        for lab in label_set:\n",
        "            mask = (y_train == lab) & (X_train_raw[:, j] != '?')\n",
        "            vals = X_train_raw[mask, j].astype(float)\n",
        "            class_means[lab][j] = float(vals.mean()) if vals.size > 0 else 0.0\n",
        "\n",
        "        missing_mask_train = (X_train_raw[:, j] == '?')\n",
        "        for i in np.where(missing_mask_train)[0]:\n",
        "            lab = y_train[i]\n",
        "            X_train_raw[i, j] = str(class_means[lab][j])\n",
        "\n",
        "        missing_mask_test = (X_test_raw[:, j] == '?')\n",
        "        for i in np.where(missing_mask_test)[0]:\n",
        "            lab = y_test[i]\n",
        "            if lab in class_means:\n",
        "                X_test_raw[i, j] = str(class_means[lab][j])\n",
        "            else:\n",
        "                vals_all = X_train_raw[X_train_raw[:, j] != '?', j].astype(float)\n",
        "                X_test_raw[i, j] = str(float(vals_all.mean()) if vals_all.size > 0 else 0.0)\n",
        "\n",
        "    # Encode categorical variables (map each category to an integer, per feature, using training)\n",
        "    X_train_enc = X_train_raw.copy()\n",
        "    X_test_enc = X_test_raw.copy()\n",
        "\n",
        "    for j in categorical_idx:\n",
        "        categories = np.unique(X_train_enc[:, j])\n",
        "        mapping = {cat: idx for idx, cat in enumerate(categories)}\n",
        "        X_train_enc[:, j] = np.vectorize(lambda v: mapping[v])(X_train_enc[:, j])\n",
        "        def map_test(v):\n",
        "            return mapping[v] if v in mapping else len(mapping)\n",
        "        X_test_enc[:, j] = np.vectorize(map_test)(X_test_enc[:, j])\n",
        "\n",
        "    X_train = X_train_enc.astype(float)\n",
        "    X_test = X_test_enc.astype(float)\n",
        "    X_train, X_test = z_normalize(X_train, X_test, numerical_idx)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "def z_normalize(X_train, X_test, feature_indices):\n",
        "    \"\"\"\n",
        "    Apply z-score normalization to specified features.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_train, X_test : numpy arrays\n",
        "    feature_indices : list of indices for numerical features\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    X_train_normalized, X_test_normalized : numpy arrays\n",
        "    \"\"\"\n",
        "    # TODO: Implement z-normalization\n",
        "    X_train_norm = X_train.copy().astype(float)\n",
        "    X_test_norm = X_test.copy().astype(float)\n",
        "\n",
        "    for j in feature_indices:\n",
        "        mu = X_train_norm[:, j].mean()\n",
        "        sigma = X_train_norm[:, j].std()\n",
        "        if sigma == 0:\n",
        "            sigma = 1.0\n",
        "        X_train_norm[:, j] = (X_train_norm[:, j] - mu) / sigma\n",
        "        X_test_norm[:, j] = (X_test_norm[:, j] - mu) / sigma\n",
        "\n",
        "    return X_train_norm, X_test_norm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2: Implement k-NN Classifier\n",
        "Implement k-NN with L2 (Euclidean) distance:\n",
        "$$\\mathcal{D}_{L2}(\\mathbf{a}, \\mathbf{b}) = \\sqrt{\\sum_i (a_i - b_i)^2}$$\n",
        "\n",
        "For **categorical attributes**, use:\n",
        "- Distance = 1 if values are different\n",
        "- Distance = 0 if values are the same\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def l2_distance(a, b):\n",
        "    \"\"\"\n",
        "    Compute L2 (Euclidean) distance between two vectors.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    a, b : numpy arrays of same shape\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    distance : float\n",
        "    \"\"\"\n",
        "    # TODO: Implement L2 distance\n",
        "    a = np.asarray(a)\n",
        "    b = np.asarray(b)\n",
        "\n",
        "    if a.shape[0] == 15 and b.shape[0] == 15:\n",
        "        numerical_idx = [1, 2, 7, 10, 13, 14]\n",
        "        categorical_idx = [i for i in range(15) if i not in numerical_idx]\n",
        "        dist_sq = 0.0\n",
        "        # numerical part\n",
        "        for j in numerical_idx:\n",
        "            dist_sq += (a[j] - b[j]) ** 2\n",
        "        # categorical part\n",
        "        for j in categorical_idx:\n",
        "            dist_sq += 0.0 if a[j] == b[j] else 1.0\n",
        "        return float(np.sqrt(dist_sq))\n",
        "    return float(np.sqrt(np.sum((a - b) ** 2)))\n",
        "\n",
        "\n",
        "def knn_predict(X_train, y_train, X_test, k):\n",
        "    \"\"\"\n",
        "    Predict labels for test data using k-NN.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_train : numpy array of shape (n_train, n_features)\n",
        "    y_train : numpy array of shape (n_train,)\n",
        "    X_test : numpy array of shape (n_test, n_features)\n",
        "    k : int, number of neighbors\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    predictions : numpy array of shape (n_test,)\n",
        "    \"\"\"\n",
        "    # TODO: Implement k-NN prediction\n",
        "    # For each test sample:\n",
        "    #   1. Compute distance to all training samples\n",
        "    #   2. Find k nearest neighbors\n",
        "    #   3. Predict using majority voting\n",
        "    X_train = np.asarray(X_train)\n",
        "    X_test = np.asarray(X_test)\n",
        "    y_train = np.asarray(y_train)\n",
        "\n",
        "    predictions = []\n",
        "    for x in X_test:\n",
        "        dists = []\n",
        "        for i in range(X_train.shape[0]):\n",
        "            d = l2_distance(X_train[i], x)\n",
        "            dists.append((d, y_train[i]))\n",
        "\n",
        "        dists.sort(key=lambda t: t[0])\n",
        "        neighbors = dists[:k]\n",
        "        votes = [lab for _, lab in neighbors]\n",
        "\n",
        "        pred = Counter(votes).most_common(1)[0][0]\n",
        "        predictions.append(pred)\n",
        "    return np.array(predictions)\n",
        "\n",
        "\n",
        "def compute_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute classification accuracy.\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    accuracy : float (between 0 and 1)\n",
        "    \"\"\"\n",
        "    # TODO: Implement accuracy computation\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    return float(np.mean(y_true == y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: Evaluate on Lenses Dataset\n",
        "Test your k-NN implementation on the Lenses dataset for different values of k.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k=1: Accuracy = 1.0000\n",
            "k=3: Accuracy = 1.0000\n",
            "k=5: Accuracy = 1.0000\n",
            "k=7: Accuracy = 1.0000\n"
          ]
        }
      ],
      "source": [
        "# TODO: Evaluate k-NN on Lenses dataset\n",
        "# Try different values of k (e.g., 1, 3, 5, 7)\n",
        "\n",
        "k_values = [1, 3, 5, 7]\n",
        "lenses_results = []\n",
        "\n",
        "for k in k_values:\n",
        "    predictions = knn_predict(X_train_lenses, y_train_lenses, X_test_lenses, k)\n",
        "    accuracy = compute_accuracy(y_test_lenses, predictions)\n",
        "    lenses_results.append((k, accuracy))\n",
        "    print(f\"k={k}: Accuracy = {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4: Evaluate on Credit Approval Dataset\n",
        "First preprocess the data, then evaluate k-NN.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Preprocess Credit Approval data\n",
        "X_train_credit, y_train_credit, X_test_credit, y_test_credit = preprocess_credit_data(\n",
        "    DATA_PATH + \"crx.data.training\",\n",
        "    DATA_PATH + \"crx.data.testing\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k=1: Accuracy = 0.8116\n",
            "k=3: Accuracy = 0.8478\n",
            "k=5: Accuracy = 0.8333\n",
            "k=7: Accuracy = 0.8478\n"
          ]
        }
      ],
      "source": [
        "# TODO: Evaluate k-NN on Credit Approval dataset\n",
        "k_values = [1, 3, 5, 7]\n",
        "credit_results = []\n",
        "\n",
        "for k in k_values:\n",
        "    predictions = knn_predict(X_train_credit, y_train_credit, X_test_credit, k)\n",
        "    accuracy = compute_accuracy(y_test_credit, predictions)\n",
        "    credit_results.append((k, accuracy))\n",
        "    print(f\"k={k}: Accuracy = {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Discussion\n",
        "\n",
        "### Results Table\n",
        "\n",
        "| Dataset | k=1 | k=3 | k=5 | k=7 |\n",
        "|---------|-----|-----|-----|-----|\n",
        "| Lenses | 1.0000 | 1.0000 | 1.0000 | 1.0000 |\n",
        "| Credit Approval | 0.8116 | 0.8478 | 0.8333 | 0.8478 |\n",
        "\n",
        "### Discussion\n",
        "*Answer these questions:*\n",
        "1. Which value of k works best for each dataset? Why do you think that is?\n",
        "\n",
        "    From the results, k = 3 and k = 7 achieve the highest accuracy (0.8478) on the Credit Approval dataset. Among them, k = 3 is preferred because it achieves high accuracy while using fewer neighbors, making it less likely to oversmooth class boundaries.\n",
        "\n",
        "2. How did preprocessing affect your results on the Credit Approval dataset?\n",
        "\n",
        "    Increasing k does not guarantee better performance in kNN. As k becomes larger, the classifier considers more distant neighbors, which may belong to different classes. This can introduce incorrect votes and reduce accuracy.\n",
        "\n",
        "3. What are the trade-offs of using different values of k?\n",
        "\n",
        "    The Lenses dataset is very small and has well-separated classes. Because kNN is a memory-based classifier, it can easily memorize the training samples and correctly classify the test samples, resulting in perfect accuracy for all tested values of k.\n",
        "\n",
        "4. What did you learn from this exercise?\n",
        "\n",
        "    This exercise helped me better understand how k-Nearest Neighbor works in practice, especially the impact of the parameter k on model performance. By testing different values of k, I observed that accuracy does not necessarily improve monotonically as k increases. \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
